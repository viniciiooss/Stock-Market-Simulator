# Projeto de An√°lise e Processamento de Dados do S&P 500 com Kafka e AWS

Este projeto demonstra um pipeline de ingest√£o, processamento e an√°lise de dados financeiros usando Apache Kafka e diversos servi√ßos da AWS, como Glue, Athena, EC2 e S3. O objetivo principal √© coletar dados do mercado financeiro (S&P 500), process√°-los em tempo real e disponibiliz√°-los para consultas e an√°lises.

## üóÇ Vis√£o Geral

O pipeline de dados foi desenvolvido para capturar informa√ß√µes do mercado financeiro, armazen√°-las no Kafka e realizar transforma√ß√µes utilizando ferramentas como o AWS Glue. Os dados processados s√£o armazenados no Amazon S3 e podem ser consultados utilizando o AWS Athena. Um Jupyter Notebook foi utilizado para explorar, visualizar e analisar os dados.

## ‚öôÔ∏è Arquitetura do Projeto

Componentes Principais
- Apache Kafka:
Usado para ingest√£o e processamento de dados em tempo real.
O notebook KafkaProducer.ipynb envia os dados de mercado para o Kafka.
O notebook KafkaConsumer.ipynb consome os dados e os armazena em uma estrutura apropriada para processamento.
- AWS Glue:
Utilizado para realizar o ETL (Extra√ß√£o, Transforma√ß√£o e Carga) dos dados do S3 para prepara√ß√£o e cataloga√ß√£o.
- AWS S3:
Armazenamento dos dados brutos e transformados.
- AWS Athena:
Realiza consultas SQL nos dados armazenados no S3, facilitando a an√°lise e gera√ß√£o de relat√≥rios.
- AWS EC2:
Hospeda o servidor Kafka e gerencia a comunica√ß√£o entre produtores e consumidores.
- Jupyter Notebook:
Utilizado para desenvolver scripts de ingest√£o e consumo de dados, assim como para an√°lises explorat√≥rias e visualiza√ß√µes dos resultados.
## üóÉÔ∏è Dados Utilizados

O projeto utiliza um conjunto de dados hist√≥ricos do S&P 500 (sp500_data_2021_to_2024.csv) que cont√©m informa√ß√µes financeiras de 2021 a 2024. As informa√ß√µes s√£o atualizadas em tempo real e enviadas ao Kafka, onde s√£o processadas e disponibilizadas para consulta no Athena.

## üîß Passos de Configura√ß√£o

- Configura√ß√£o do Kafka:
Configura√ß√£o de t√≥picos para envio e consumo de dados.
Definir a estrutura dos dados para serem processados pelo consumidor.
- Configura√ß√£o dos Servi√ßos AWS:
Criar buckets no S3 para armazenamento dos dados brutos e processados.
Definir Glue Crawlers para catalogar os dados e criar tabelas.
Criar consultas no Athena para acessar os dados transformados.
- Execu√ß√£o dos Notebooks:
KafkaProducer.ipynb: Respons√°vel por enviar dados para o Kafka.
KafkaConsumer.ipynb: Consome os dados do Kafka e realiza transforma√ß√µes b√°sicas.
## üìä Consultas no Athena

A partir das imagens fornecidas, foram executadas as seguintes consultas no AWS Athena:

Consulta de contagem de registros:
```bash
SELECT COUNT(*) FROM "stock-market-kafka"."kafka_stock_sp500" LIMIT 10;
```
Resultado: 226 registros.

Consulta da √∫ltima data no dataset:
```bash
SELECT MAX(DATE) FROM "stock-market-kafka"."kafka_stock_sp500" LIMIT 10;
```

Resultado: Data mais recente ‚Äî 2024-09-25.
Essas consultas confirmam a efic√°cia do pipeline de dados para armazenar e consultar informa√ß√µes atualizadas do mercado financeiro.

## üöÄ Execu√ß√£o

Para executar o projeto, siga as etapas descritas nos notebooks (KafkaProducer.ipynb e KafkaConsumer.ipynb) e consulte os dados no Athena conforme as queries fornecidas. Certifique-se de que todas as depend√™ncias estejam configuradas corretamente e os servi√ßos AWS estejam em execu√ß√£o.

## üìà Resultados

Ingest√£o de Dados em Tempo Real: Os dados do mercado financeiro s√£o capturados em tempo real e processados pelo pipeline.
Armazenamento Eficiente: Dados armazenados no S3 e consult√°veis via Athena.

## üìú Licen√ßa

Este projeto est√° licenciado sob os termos da licen√ßa MIT.
